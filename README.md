# BnB Data4Transformation - Political Data ELT Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Data Sources  â”‚    â”‚   Extraction    â”‚    â”‚   Raw Storage   â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ DAWUM API     â”‚â”€â”€â”€â–¶â”‚ â€¢ Connectors    â”‚â”€â”€â”€â–¶â”‚ â€¢ MinIO/S3      â”‚
â”‚ â€¢ Destatis API  â”‚    â”‚ â€¢ Scrapy        â”‚    â”‚ â€¢ JSON/Parquet  â”‚
â”‚ â€¢ Eurostat API  â”‚    â”‚ â€¢ BeautifulSoup â”‚    â”‚ â€¢ ClickHouse    â”‚
â”‚ â€¢ GESIS API     â”‚    â”‚ â€¢ Async/Batch   â”‚    â”‚   raw schema    â”‚
â”‚ â€¢ SOEP API      â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ Web Scraping  â”‚    â”‚                 â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Orchestration  â”‚    â”‚ Transformation  â”‚    â”‚  Analytics DWH  â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ Airflow 2.9+  â”‚â”€â”€â”€â–¶â”‚ â€¢ dbt-clickhouseâ”‚â”€â”€â”€â–¶â”‚ â€¢ ClickHouse    â”‚
â”‚ â€¢ DAG Scheduler â”‚    â”‚ â€¢ SQL Models    â”‚    â”‚ â€¢ Star Schema   â”‚
â”‚ â€¢ Task Sensors  â”‚    â”‚ â€¢ Tests & Docs  â”‚    â”‚ â€¢ Marts/Cubes   â”‚
â”‚ â€¢ Monitoring    â”‚    â”‚ â€¢ Incremental   â”‚    â”‚ â€¢ Data Quality  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ¯ Overview

Production-grade ELT pipeline for political polling and socioeconomic data aggregation. Built with modern Python, Airflow orchestration, ClickHouse analytics warehouse, and dbt transformations.

### Key Features
- **Async API Connectors**: DAWUM, Destatis, Eurostat, GESIS, SOEP
- **Web Scraping**: Scrapy framework for sites without APIs
- **Object Storage**: MinIO S3-compatible for raw data lake
- **Analytics Warehouse**: ClickHouse for high-performance analytics
- **Transform Layer**: dbt-core with ClickHouse adapter
- **Orchestration**: Apache Airflow with TaskFlow API
- **Monitoring**: Prometheus metrics + data quality tests
- **Development**: Poetry, pre-commit, pytest, type hints

## ğŸš€ Quick Start

### Prerequisites
- Docker & Docker Compose
- Python 3.11+
- Poetry (or use provided Makefile)

### 1. Initialize Project
```bash
# Clone and setup
git clone <repository>
cd bnb_data4transformation

# Install dependencies and hooks
make init

# Copy environment template
cp .env.example .env
# Edit .env with your API keys and credentials
```

### 2. Start Infrastructure
```bash
# Start all services (ClickHouse, Airflow, MinIO, etc.)
make up && make smoke   # start stack and verify health

# Check service health
make status

# Initialize ClickHouse schemas
make init-db

# Access services:
# - Airflow UI: http://localhost:8081 (airflow/airflow)
# - ClickHouse: http://localhost:8124
# - MinIO Console: http://localhost:9003 (minioadmin/minioadmin)
# - MinIO API: http://localhost:9002
# - Jupyter: http://localhost:8888?token=admin
```

### 3. Run Sample Pipeline
```bash
# Load sample DAWUM data
make load-local

# Run dbt models
make dbt-run

# Access services:
# - Airflow UI: http://localhost:8081 (airflow/airflow)
# - ClickHouse: http://localhost:8124
# - MinIO Console: http://localhost:9003 (minioadmin/minioadmin)
# - MinIO API: http://localhost:9002
# - Jupyter: http://localhost:8888?token=admin
```

## ğŸ“ Project Structure

```
â”œâ”€â”€ README.md                 # This file
â”œâ”€â”€ docker-compose.yml        # Infrastructure orchestration
â”œâ”€â”€ .env.example              # Environment variables template
â”œâ”€â”€ pyproject.toml            # Python dependencies & tools
â”œâ”€â”€ Makefile                  # Common development tasks
â”œâ”€â”€ 
â”œâ”€â”€ dags/                     # Airflow DAG definitions
â”‚   â”œâ”€â”€ extract_api_dag.py    # API data extraction
â”‚   â”œâ”€â”€ extract_scrape_dag.py # Web scraping jobs
â”‚   â”œâ”€â”€ load_clickhouse_dag.py # Data loading
â”‚   â”œâ”€â”€ dbt_transform_dag.py  # dbt transformations
â”‚   â””â”€â”€ dbt_transformation_dag.py # dbt orchestration
â”œâ”€â”€ 
â”œâ”€â”€ connectors/               # Data source connectors
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_connector.py     # Abstract base class
â”‚   â”œâ”€â”€ dawum_connector.py    # DAWUM API client
â”‚   â”œâ”€â”€ destatis_connector.py # Destatis API client  
â”‚   â”œâ”€â”€ eurostat_connector.py # Eurostat API client
â”‚   â”œâ”€â”€ gesis_connector.py    # GESIS API client
â”‚   â””â”€â”€ soep_connector.py     # SOEP API client
â”œâ”€â”€ 
â”œâ”€â”€ elt/                      # Extract-Load-Transform utilities
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ loader_clickhouse.py  # ClickHouse bulk loader
â”‚   â”œâ”€â”€ metadata.py          # Data schemas & models
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ logging_config.py # Structured logging
â”‚       â””â”€â”€ persistence.py   # File I/O helpers
â”œâ”€â”€ 
â”œâ”€â”€ scraping/                 # Web scraping framework
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ settings.py          # Scrapy configuration
â”‚   â””â”€â”€ spiders/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ example_spider.py # Template spider
â”œâ”€â”€ 
â”œâ”€â”€ dbt_project/             # dbt transformation project
â”‚   â”œâ”€â”€ dbt_project.yml      # dbt configuration
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ raw/             # Raw data models
â”‚   â”‚   â”œâ”€â”€ staging/         # Staging transformations
â”‚   â”‚   â””â”€â”€ marts/           # Business logic models
â”‚   â”œâ”€â”€ macros/              # Reusable SQL macros
â”‚   â”œâ”€â”€ seeds/               # Static reference data
â”‚   â””â”€â”€ tests/               # Data quality tests
â”œâ”€â”€ 
â”œâ”€â”€ tests/                   # Test suite
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ unit/                # Unit tests
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ test_connectors.py
â”‚   â””â”€â”€ integration/         # Integration tests
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ test_pipeline.py
â”œâ”€â”€ 
â”œâ”€â”€ notebooks/               # Jupyter analysis notebooks
â”œâ”€â”€ data/                    # Local data storage
â”‚   â”œâ”€â”€ raw/                 # Raw extracted data
â”‚   â”œâ”€â”€ logs/                # Application logs
â”‚   â””â”€â”€ exports/             # Analysis outputs
â”œâ”€â”€ 
â”œâ”€â”€ scripts/                 # Utility scripts
â”‚   â”œâ”€â”€ check_stack.sh       # Bash smoke tests
â”‚   â”œâ”€â”€ check_stack.ps1      # PowerShell smoke tests
â”‚   â”œâ”€â”€ init_clickhouse.sql  # Database initialization
â”‚   â””â”€â”€ setup_dev.py         # Development setup
â”œâ”€â”€ 
â”œâ”€â”€ config/                  # Configuration files
â”‚   â”œâ”€â”€ airflow.cfg          # Airflow configuration
â”‚   â””â”€â”€ logging.conf         # Logging configuration
â””â”€â”€ 
â””â”€â”€ .github/                 # CI/CD workflows
    â””â”€â”€ workflows/
        â””â”€â”€ ci.yml           # GitHub Actions CI
```

## ğŸ”§ Development

### Common Commands
```bash
# Development setup
make init                # Install dependencies & pre-commit hooks
make dev                 # Start development environment
make test                # Run test suite
make lint                # Run code quality checks
make format              # Format code

# Data pipeline
make extract             # Run data extraction
make load                # Load data to ClickHouse
make transform           # Run dbt transformations
make dbt-run             # Run dbt models
make dbt-version         # Check dbt version and connectivity
make pipeline            # Run full ELT pipeline
make smoke               # Run health checks

# Infrastructure
make up                  # Start all services
make down                # Stop all services
make logs                # View service logs
make reset               # Reset all data & containers
```

### Testing
```bash
# Run all tests
pytest

# Run specific test categories
pytest -m unit           # Unit tests only
pytest -m integration    # Integration tests only
pytest -m "not slow"     # Skip slow tests

# With coverage
pytest --cov=connectors --cov=elt --cov-report=html
```

### Code Quality
Pre-commit hooks automatically run:
- **Black**: Code formatting
- **Ruff**: Fast linting
- **isort**: Import sorting
- **mypy**: Type checking
- **pytest**: Test execution

## ğŸ—„ï¸ Data Architecture

### Raw Layer (`raw` schema)
- **raw_ingestions**: Metadata about data extractions
- **raw_dawum_polls**: DAWUM polling data (JSON)
- **raw_destatis_***: Destatis datasets
- **raw_eurostat_***: Eurostat datasets
- **raw_web_scrapes**: Scraped content

### Staging Layer (`staging` schema)
- **stg_polls**: Cleaned polling data
- **stg_demographics**: Demographic indicators
- **stg_economics**: Economic indicators
- **stg_elections**: Election results

### Marts Layer (`analytics` schema)
- **dim_parties**: Political parties dimension
- **dim_institutes**: Polling institutes dimension
- **dim_geography**: Geographic dimension
- **fact_polls**: Polling facts table
- **fact_elections**: Election results facts

## ğŸ“Š Monitoring & Data Quality

### Airflow Monitoring
- Task success/failure rates
- DAG run duration
- Data freshness SLAs
- Custom metrics via Prometheus

### dbt Testing
- Not-null constraints
- Uniqueness checks
- Referential integrity
- Custom business rules
- Freshness assertions

### Alerts
- Slack notifications for failures
- Email alerts for data quality issues
- Prometheus alerts for system metrics

## ğŸ”‘ Configuration

### Environment Variables
See `.env.example` for complete configuration options:
- **Database**: ClickHouse connection settings
- **APIs**: Authentication keys for data sources
- **Storage**: MinIO/S3 credentials
- **Monitoring**: Slack webhook URLs
- **Development**: Debug flags, log levels

### Airflow Connections
Configure via Airflow UI or environment variables:
- `clickhouse_default`: ClickHouse connection
- `minio_default`: Object storage connection
- `slack_default`: Slack notifications

## ğŸš€ Deployment

### Production Considerations
- Use external PostgreSQL for Airflow metadata
- Configure ClickHouse cluster for high availability
- Set up proper SSL certificates
- Use Kubernetes for container orchestration
- Implement proper secrets management
- Set up automated backups

### CI/CD Pipeline
GitHub Actions workflow includes:
- Dependency installation
- Code quality checks
- Test execution
- dbt model compilation
- Docker image building
- Deployment to staging/production

## ğŸ“ˆ Performance Optimization

### ClickHouse Tuning
- Use appropriate table engines (MergeTree, ReplacingMergeTree)
- Optimize partition keys and order by clauses
- Configure proper compression settings
- Use materialized views for aggregations

### Pipeline Optimization
- Implement incremental data loading
- Use connection pooling for APIs
- Batch processing for large datasets
- Async/await patterns for I/O operations

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Run tests and quality checks
5. Submit a pull request

## ğŸ“ License

MIT License - see LICENSE file for details.
