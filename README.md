# BnB Data4Transformation - Political Data ELT Pipeline

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Data Sources  ‚îÇ    ‚îÇ   Extraction    ‚îÇ    ‚îÇ   Raw Storage   ‚îÇ
‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ
‚îÇ ‚Ä¢ DAWUM API     ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ ‚Ä¢ Connectors    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ ‚Ä¢ MinIO/S3      ‚îÇ
‚îÇ ‚Ä¢ Destatis API  ‚îÇ    ‚îÇ ‚Ä¢ Scrapy        ‚îÇ    ‚îÇ ‚Ä¢ JSON/Parquet  ‚îÇ
‚îÇ ‚Ä¢ Eurostat API  ‚îÇ    ‚îÇ ‚Ä¢ BeautifulSoup ‚îÇ    ‚îÇ ‚Ä¢ ClickHouse    ‚îÇ
‚îÇ ‚Ä¢ GESIS API     ‚îÇ    ‚îÇ ‚Ä¢ Async/Batch   ‚îÇ    ‚îÇ   raw schema    ‚îÇ
‚îÇ ‚Ä¢ SOEP API      ‚îÇ    ‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ
‚îÇ ‚Ä¢ Web Scraping  ‚îÇ    ‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                       ‚îÇ                       ‚îÇ
         ‚îÇ                       ‚îÇ                       ‚îÇ
         ‚ñº                       ‚ñº                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Orchestration  ‚îÇ    ‚îÇ Transformation  ‚îÇ    ‚îÇ  Analytics DWH  ‚îÇ
‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ
‚îÇ ‚Ä¢ Airflow 2.9+  ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ ‚Ä¢ dbt-clickhouse‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ ‚Ä¢ ClickHouse    ‚îÇ
‚îÇ ‚Ä¢ DAG Scheduler ‚îÇ    ‚îÇ ‚Ä¢ SQL Models    ‚îÇ    ‚îÇ ‚Ä¢ Star Schema   ‚îÇ
‚îÇ ‚Ä¢ Task Sensors  ‚îÇ    ‚îÇ ‚Ä¢ Tests & Docs  ‚îÇ    ‚îÇ ‚Ä¢ Marts/Cubes   ‚îÇ
‚îÇ ‚Ä¢ Monitoring    ‚îÇ    ‚îÇ ‚Ä¢ Incremental   ‚îÇ    ‚îÇ ‚Ä¢ Data Quality  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üéØ Overview

Production-grade ELT pipeline for political polling and socioeconomic data aggregation. Built with modern Python, Airflow orchestration, ClickHouse analytics warehouse, and dbt transformations.

### Key Features
- **Async API Connectors**: DAWUM ‚úÖ, Destatis üöß, Eurostat, GESIS ‚úÖ, SOEP
- **Web Scraping**: Scrapy framework for sites without APIs
- **Object Storage**: MinIO S3-compatible for raw data lake
- **Analytics Warehouse**: ClickHouse for high-performance analytics
- **Transform Layer**: dbt-core with ClickHouse adapter
- **Orchestration**: Apache Airflow with TaskFlow API
- **Monitoring**: Prometheus metrics + data quality tests
- **Development**: Poetry, pre-commit, pytest, type hints

> üìã **Connector Status**: See [Destatis Development Status](docs/DESTATIS_STATUS_TODOS.md) for detailed TODO list and implementation progress.

## üöÄ Quick Start

### Prerequisites
- Docker & Docker Compose
- Python 3.11+
- Poetry (or use provided Makefile)

### 1. Start Docker Desktop
```powershell
# Start Docker Desktop on Windows
Start-Process "C:\Program Files\Docker\Docker\Docker Desktop.exe"

# Wait for Docker to be ready (30-60 seconds)
# You can verify with: docker info
```

### 2. Initialize Project
```bash
# Clone and setup
git clone <repository>
cd bnb_data4transformation

# Install dependencies and hooks
make init

# Copy environment template
cp .env.example .env
# Edit .env with your API keys and credentials
```

### 3. Start Infrastructure
start docker desktop, and conda activate bnb_data4transformation
```bash
# Start all services (ClickHouse, Airflow, MinIO, etc.)
make up && make smoke   # start stack and verify health

# Check service health
make status

# Initialize ClickHouse schemas
make init-db

# Access services:
# - Airflow UI: http://localhost:8081 (airflow/airflow)
# - ClickHouse: http://localhost:8124
# - MinIO Console: http://localhost:9003 (minioadmin/minioadmin)
# - MinIO API: http://localhost:9002
# - Jupyter: http://localhost:8888?token=admin
```

### 4. Run Sample Pipeline
```bash
# Load sample DAWUM data
make load-local

# Extract Destatis statistical data (using new POST API since July 2025)
make extract-destatis TABLES="12411-0001,12411-0002" AREA="de" START_YEAR="2020" END_YEAR="2023"

# Test with specific credentials for large datasets
DESTATIS_USER=<REDACTED> DESTATIS_PASS=<REDACTED> make extract-destatis

# Run dbt models
make dbt-run

# Access services:
# - Airflow UI: http://localhost:8081 (airflow/airflow)
# - ClickHouse: http://localhost:8124
# - MinIO Console: http://localhost:9003 (minioadmin/minioadmin)
# - MinIO API: http://localhost:9002
# - Jupyter: http://localhost:8888?token=admin
```

### 5. Verify Data Loading
```bash
# Check ClickHouse data ingestion
docker exec clickhouse clickhouse-client --query "SELECT COUNT(*) as poll_count FROM raw.dawum_polls"
# Expected: ~3,500+ polling records

# View recent polls
docker exec clickhouse clickhouse-client --query "SELECT poll_id, publication_date, sample_size FROM raw.dawum_polls ORDER BY publication_date DESC LIMIT 5"

# Check all available tables
docker exec clickhouse clickhouse-client --query "SHOW TABLES FROM raw"
```

## üìä Data Sources

### DAWUM Polling API ‚úÖ
German polling data aggregator providing real-time political polling data.

**API Status:** Fully operational (updated July 23, 2025)
- ‚úÖ API endpoint updated from `/data.json` to root endpoint `/`
- ‚úÖ 3,500+ polls successfully ingested into ClickHouse
- ‚úÖ Real-time data extraction with async HTTP client
- ‚úÖ Full data transformation pipeline operational

**Features:**
- Anonymous access (no authentication required)
- JSON format with structured polling data
- Institutes, parties, and parliament metadata
- Rate limiting and retry logic implemented

### GESIS Knowledge Graph API ‚úÖ
GESIS - Leibniz Institute for the Social Sciences provides comprehensive social science research datasets through their SPARQL endpoint.

**API Status:** Production-ready (updated August 12, 2025)
- ‚úÖ Full pipeline operational with 9,176+ datasets
- ‚úÖ SPARQL endpoint extraction with complete RDF metadata
- ‚úÖ Optimized batch processing (20 datasets/batch)
- ‚úÖ Rich metadata extraction (~40 properties per dataset)
- ‚úÖ 100% data completeness with robust error handling

**Features:**
- SPARQL endpoint with schema:Dataset queries
- Complete RDF triple extraction for comprehensive metadata
- Batch processing to handle large dataset catalogs
- Anonymous access (no authentication required)
- Automatic retry logic for failed batches

**Usage:**
```bash
# Trigger GESIS metadata extraction DAG
airflow dags trigger fetch_gesis_metadata

# Check extracted data in ClickHouse
docker exec clickhouse clickhouse-client --query "SELECT COUNT(*) FROM raw.gesis_metadata"
# Expected: ~9,176+ research datasets
```

### GENESIS-Online (Destatis) API
The German Federal Statistical Office provides comprehensive statistical data through their GENESIS-Online REST API.


**Example API Usage:**
```bash
# Test table extraction
curl -X GET "https://www-genesis.destatis.de/genesisWS/rest/2020/data/table" \
  -H "Authorization: Basic $(echo -n 'user:pass' | base64)" \
  -G -d "name=12411-0001" \
  -d "area=de" \
  -d "format=json" \
  -d "compress=true"

# Extract population statistics
make extract-destatis TABLES="12411-0001,12411-0002" AREA="de" START_YEAR="2020" END_YEAR="2023"
```

**Supported Features:**
- Anonymous access for small tables (<50MB)
- Authenticated access for large datasets
- Auto-chunking for tables >10MB or >1M cells
- Multiple formats: JSON-stat, SDMX-JSON, CSV
- Transparent gzip decompression
- Exponential backoff retry on HTTP 5xx/429
- Rate limiting (30 requests/minute)

## üìÅ Project Structure

```
‚îú‚îÄ‚îÄ README.md                 # This file
‚îú‚îÄ‚îÄ docker-compose.yml        # Infrastructure orchestration
‚îú‚îÄ‚îÄ .env.example              # Environment variables template
‚îú‚îÄ‚îÄ pyproject.toml            # Python dependencies & tools
‚îú‚îÄ‚îÄ Makefile                  # Common development tasks
‚îú‚îÄ‚îÄ 
‚îú‚îÄ‚îÄ dags/                     # Airflow DAG definitions
‚îÇ   ‚îú‚îÄ‚îÄ extract_api_dag.py    # API data extraction
‚îÇ   ‚îú‚îÄ‚îÄ extract_scrape_dag.py # Web scraping jobs
‚îÇ   ‚îú‚îÄ‚îÄ load_clickhouse_dag.py # Data loading
‚îÇ   ‚îú‚îÄ‚îÄ dbt_transform_dag.py  # dbt transformations
‚îÇ   ‚îî‚îÄ‚îÄ dbt_transformation_dag.py # dbt orchestration
‚îú‚îÄ‚îÄ 
‚îú‚îÄ‚îÄ connectors/               # Data source connectors
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ base_connector.py     # Abstract base class
‚îÇ   ‚îú‚îÄ‚îÄ dawum_connector.py    # DAWUM API client
‚îÇ   ‚îú‚îÄ‚îÄ destatis_connector.py # GENESIS-Online POST API client (2025)
‚îÇ   ‚îú‚îÄ‚îÄ eurostat_connector.py # Eurostat API client
‚îÇ   ‚îú‚îÄ‚îÄ gesis_connector.py    # GESIS API client
‚îÇ   ‚îî‚îÄ‚îÄ soep_connector.py     # SOEP API client
‚îú‚îÄ‚îÄ 
‚îú‚îÄ‚îÄ elt/                      # Extract-Load-Transform utilities
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ loader_clickhouse.py  # ClickHouse bulk loader
‚îÇ   ‚îú‚îÄ‚îÄ metadata.py          # Data schemas & models
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ logging_config.py # Structured logging
‚îÇ       ‚îî‚îÄ‚îÄ persistence.py   # File I/O helpers
‚îú‚îÄ‚îÄ 
‚îú‚îÄ‚îÄ scraping/                 # Web scraping framework
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ settings.py          # Scrapy configuration
‚îÇ   ‚îî‚îÄ‚îÄ spiders/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îî‚îÄ‚îÄ example_spider.py # Template spider
‚îú‚îÄ‚îÄ 
‚îú‚îÄ‚îÄ dbt_project/             # dbt transformation project
‚îÇ   ‚îú‚îÄ‚îÄ dbt_project.yml      # dbt configuration
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ raw/             # Raw data models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ staging/         # Staging transformations
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ marts/           # Business logic models
‚îÇ   ‚îú‚îÄ‚îÄ macros/              # Reusable SQL macros
‚îÇ   ‚îú‚îÄ‚îÄ seeds/               # Static reference data
‚îÇ   ‚îî‚îÄ‚îÄ tests/               # Data quality tests
‚îú‚îÄ‚îÄ 
‚îú‚îÄ‚îÄ tests/                   # Test suite
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ unit/                # Unit tests
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_connectors.py
‚îÇ   ‚îî‚îÄ‚îÄ integration/         # Integration tests
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îî‚îÄ‚îÄ test_pipeline.py
‚îú‚îÄ‚îÄ 
‚îú‚îÄ‚îÄ notebooks/               # Jupyter analysis notebooks
‚îú‚îÄ‚îÄ data/                    # Local data storage
‚îÇ   ‚îú‚îÄ‚îÄ raw/                 # Raw extracted data
‚îÇ   ‚îú‚îÄ‚îÄ logs/                # Application logs
‚îÇ   ‚îî‚îÄ‚îÄ exports/             # Analysis outputs
‚îú‚îÄ‚îÄ 
‚îú‚îÄ‚îÄ scripts/                 # Utility scripts
‚îÇ   ‚îú‚îÄ‚îÄ check_stack.sh       # Bash smoke tests
‚îÇ   ‚îú‚îÄ‚îÄ check_stack.ps1      # PowerShell smoke tests
‚îÇ   ‚îú‚îÄ‚îÄ init_clickhouse.sql  # Database initialization
‚îÇ   ‚îî‚îÄ‚îÄ setup_dev.py         # Development setup
‚îú‚îÄ‚îÄ 
‚îú‚îÄ‚îÄ config/                  # Configuration files
‚îÇ   ‚îú‚îÄ‚îÄ airflow.cfg          # Airflow configuration
‚îÇ   ‚îî‚îÄ‚îÄ logging.conf         # Logging configuration
‚îî‚îÄ‚îÄ 
‚îî‚îÄ‚îÄ .github/                 # CI/CD workflows
    ‚îî‚îÄ‚îÄ workflows/
        ‚îî‚îÄ‚îÄ ci.yml           # GitHub Actions CI
```

## üîß Development

### Common Commands
```bash
# Development setup
make init                # Install dependencies & pre-commit hooks
make dev                 # Start development environment
make test                # Run test suite
make lint                # Run code quality checks
make format              # Format code

# Data pipeline
make extract             # Run data extraction
make load                # Load data to ClickHouse
make transform           # Run dbt transformations
make dbt-run             # Run dbt models
make dbt-version         # Check dbt version and connectivity
make pipeline            # Run full ELT pipeline
make smoke               # Run health checks

# Infrastructure
make up                  # Start all services
make down                # Stop all services
make logs                # View service logs
make reset               # Reset all data & containers
```

### Testing
```bash
# Run all tests
pytest

# Run specific test categories
pytest -m unit           # Unit tests only
pytest -m integration    # Integration tests only
pytest -m "not slow"     # Skip slow tests

# With coverage
pytest --cov=connectors --cov=elt --cov-report=html
```

### Code Quality
Pre-commit hooks automatically run:
- **Black**: Code formatting
- **Ruff**: Fast linting
- **isort**: Import sorting
- **mypy**: Type checking
- **pytest**: Test execution

## üóÑÔ∏è Data Architecture

### GENESIS-Online API (Destatis) - Updated July 2025

**Important**: Since July 15, 2025, the SOAP/XML interface and GET methods have been discontinued. Our connector now uses the new POST-based RESTful JSON interface.

#### API Configuration
```bash
# For large datasets (>50MB) - requires registration
DESTATIS_USER=<REDACTED>
DESTATIS_PASS=<REDACTED>

# Base URL (REST v2020 with POST methods)
DESTATIS_BASE_URL=https://www-genesis.destatis.de/genesisWS/rest/2020/
```

#### Usage Examples
```bash
# Extract multiple tables with time range
make extract-destatis TABLES="12411-0001,12411-0002,21311-0001" START_YEAR="2020" END_YEAR="2023"

# Test with specific area and format
make extract-destatis TABLES="12411-0001" AREA="de" FORMAT="csv"
```

#### API Example (POST method)
```bash
curl -X POST "https://www-genesis.destatis.de/genesisWS/rest/2020/data/table" \
  -H "Content-Type: application/json" \
  -H "Authorization: Basic $(echo -n 'user:pass' | base64)" \
  -d '{
    "name": "12411-0001",
    "area": "de", 
    "format": "json",
    "compress": "true",
    "startYear": "2020",
    "endYear": "2023"
  }'
```

#### Supported Datasets
- **12411-0001**: Population by age groups and sex
- **12411-0002**: Population by citizenship  
- **21311-0001**: Live births and deaths
- **81000-0001**: Federal election results
- And thousands more statistical tables

### Raw Layer (`raw` schema)
- **raw_ingestions**: Metadata about data extractions
- **raw_dawum_polls**: DAWUM polling data (JSON) - ‚úÖ **3,527 records verified**
- **raw_destatis_***: Destatis datasets (GENESIS-Online)
- **raw_eurostat_***: Eurostat datasets
- **raw_web_scrapes**: Scraped content

### Staging Layer (`staging` schema)
- **stg_polls**: Cleaned polling data
- **stg_demographics**: Demographic indicators
- **stg_economics**: Economic indicators
- **stg_elections**: Election results

### Marts Layer (`analytics` schema)
- **dim_parties**: Political parties dimension
- **dim_institutes**: Polling institutes dimension
- **dim_geography**: Geographic dimension
- **fact_polls**: Polling facts table
- **fact_elections**: Election results facts

## üìä Monitoring & Data Quality

### Airflow Monitoring
- Task success/failure rates
- DAG run duration
- Data freshness SLAs
- Custom metrics via Prometheus

### dbt Testing
- Not-null constraints
- Uniqueness checks
- Referential integrity
- Custom business rules
- Freshness assertions

### Alerts
- Slack notifications for failures
- Email alerts for data quality issues
- Prometheus alerts for system metrics

## üîë Configuration

### Environment Variables
See `.env.example` for complete configuration options:
- **Database**: ClickHouse connection settings
- **APIs**: Authentication keys for data sources
- **Storage**: MinIO/S3 credentials
- **Monitoring**: Slack webhook URLs
- **Development**: Debug flags, log levels

### Airflow Connections
Configure via Airflow UI or environment variables:
- `clickhouse_default`: ClickHouse connection
- `minio_default`: Object storage connection
- `slack_default`: Slack notifications

## üöÄ Deployment

### Current Infrastructure Status (July 23, 2025)
‚úÖ **All Systems Operational**
- **Docker Containers**: 8/8 healthy
- **Airflow**: Scheduler + Webserver running (LocalExecutor)
- **ClickHouse**: Database operational with data
- **DAWUM Pipeline**: Fully functional with 3,527 polls ingested
- **Destatis Connector**: Under development (authentication testing)

### Production Considerations
- Use external PostgreSQL for Airflow metadata
- Configure ClickHouse cluster for high availability
- Set up proper SSL certificates
- Use Kubernetes for container orchestration
- Implement proper secrets management
- Set up automated backups

### CI/CD Pipeline
GitHub Actions workflow includes:
- Dependency installation
- Code quality checks
- Test execution
- dbt model compilation
- Docker image building
- Deployment to staging/production

## üìà Performance Optimization

### ClickHouse Tuning
- Use appropriate table engines (MergeTree, ReplacingMergeTree)
- Optimize partition keys and order by clauses
- Configure proper compression settings
- Use materialized views for aggregations

### Pipeline Optimization
- Implement incremental data loading
- Use connection pooling for APIs
- Batch processing for large datasets
- Async/await patterns for I/O operations

## ü§ù Contributing

### Known Issues & Solutions
- **DAWUM API Endpoint Change (July 2025)**: ‚úÖ **RESOLVED** - Updated connector to use root endpoint instead of `/data.json`
- **orjson Compilation Issues**: ‚úÖ **RESOLVED** - Using binary wheels with `PIP_PREFER_BINARY=1`
- **Container Health**: ‚úÖ **VERIFIED** - All 8 containers healthy and operational

### Development Workflow
1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Run tests and quality checks
5. Submit a pull request

## üìù License

MIT License - see LICENSE file for details.
